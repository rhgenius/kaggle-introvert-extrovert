{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colab-title",
   "metadata": {
    "id": "colab-title"
   },
   "source": [
    "# Kaggle Introvert vs Extrovert Classification - Google Colab Version\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-username/your-repo/blob/main/notebooks/colab_complete_analysis.ipynb)\n",
    "\n",
    "This notebook provides a complete analysis and modeling pipeline for the Kaggle Introvert vs Extrovert classification competition, optimized for Google Colab.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Installation](#setup)\n",
    "2. [Data Upload and Loading](#data-loading)\n",
    "3. [Exploratory Data Analysis](#eda)\n",
    "4. [Feature Engineering](#feature-engineering)\n",
    "5. [Model Training and Evaluation](#model-training)\n",
    "6. [Final Predictions](#predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the required packages and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-packages",
   "metadata": {
    "id": "install-packages"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm\n",
    "!pip install -q kaggle\n",
    "\n",
    "print('Packages installed successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "import zipfile\n",
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {
    "id": "data-loading"
   },
   "source": [
    "## 2. Data Upload and Loading\n",
    "\n",
    "Choose one of the following methods to load your data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload-methods",
   "metadata": {
    "id": "upload-methods"
   },
   "source": [
    "### Method 1: Upload files directly (Recommended for small files)\n",
    "\n",
    "Run the cell below and upload your `train.csv`, `test.csv`, and `sample_submission.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-files",
   "metadata": {
    "id": "upload-files"
   },
   "outputs": [],
   "source": [
    "# Method 1: Direct file upload\n",
    "print('Please upload your CSV files (train.csv, test.csv, sample_submission.csv):')\n",
    "uploaded = files.upload()\n",
    "\n",
    "# List uploaded files\n",
    "print('\\nUploaded files:')\n",
    "for filename in uploaded.keys():\n",
    "    print(f'- {filename} ({len(uploaded[filename])} bytes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kaggle-method",
   "metadata": {
    "id": "kaggle-method"
   },
   "source": [
    "### Method 2: Download from Kaggle API (Alternative)\n",
    "\n",
    "If you prefer to download directly from Kaggle, upload your `kaggle.json` file first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kaggle-setup",
   "metadata": {
    "id": "kaggle-setup"
   },
   "outputs": [],
   "source": [
    "# Method 2: Kaggle API (uncomment if using)\n",
    "# print('Upload your kaggle.json file:')\n",
    "# uploaded = files.upload()\n",
    "# \n",
    "# # Setup Kaggle API\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# \n",
    "# # Download competition data\n",
    "# !kaggle competitions download -c playground-series-s5e7\n",
    "# !unzip -o playground-series-s5e7.zip\n",
    "# !rm playground-series-s5e7.zip\n",
    "\n",
    "print('Kaggle setup ready (if needed)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {
    "id": "load-data"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "try:\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    sample_submission = pd.read_csv('sample_submission.csv')\n",
    "    \n",
    "    print('Data loaded successfully!')\n",
    "    print(f'Training data shape: {train_df.shape}')\n",
    "    print(f'Test data shape: {test_df.shape}')\n",
    "    print(f'Sample submission shape: {sample_submission.shape}')\n",
    "except FileNotFoundError as e:\n",
    "    print(f'Error loading data: {e}')\n",
    "    print('Please make sure you have uploaded the CSV files or downloaded them via Kaggle API')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda",
   "metadata": {
    "id": "eda"
   },
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-info",
   "metadata": {
    "id": "basic-info"
   },
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print('=== Dataset Overview ===')\n",
    "print('\\nTraining Data Info:')\n",
    "print(train_df.info())\n",
    "\n",
    "print('\\nFirst few rows of training data:')\n",
    "display(train_df.head())\n",
    "\n",
    "print('\\nBasic statistics:')\n",
    "display(train_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-values",
   "metadata": {
    "id": "missing-values"
   },
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print('=== Missing Values Analysis ===')\n",
    "missing_train = train_df.isnull().sum()\n",
    "missing_test = test_df.isnull().sum()\n",
    "\n",
    "print('\\nMissing values in training data:')\n",
    "print(missing_train[missing_train > 0])\n",
    "\n",
    "print('\\nMissing values in test data:')\n",
    "print(missing_test[missing_test > 0])\n",
    "\n",
    "# Visualize missing values\n",
    "if missing_train.sum() > 0 or missing_test.sum() > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Training data missing values\n",
    "    missing_train_pct = (missing_train / len(train_df)) * 100\n",
    "    missing_train_pct = missing_train_pct[missing_train_pct > 0].sort_values(ascending=False)\n",
    "    if len(missing_train_pct) > 0:\n",
    "        missing_train_pct.plot(kind='bar', ax=axes[0])\n",
    "        axes[0].set_title('Missing Values in Training Data (%)')\n",
    "        axes[0].set_ylabel('Percentage')\n",
    "    \n",
    "    # Test data missing values\n",
    "    missing_test_pct = (missing_test / len(test_df)) * 100\n",
    "    missing_test_pct = missing_test_pct[missing_test_pct > 0].sort_values(ascending=False)\n",
    "    if len(missing_test_pct) > 0:\n",
    "        missing_test_pct.plot(kind='bar', ax=axes[1])\n",
    "        axes[1].set_title('Missing Values in Test Data (%)')\n",
    "        axes[1].set_ylabel('Percentage')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No missing values found in the datasets!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target-analysis",
   "metadata": {
    "id": "target-analysis"
   },
   "outputs": [],
   "source": [
    "# Target variable analysis\n",
    "target_col = None\n",
    "for col in train_df.columns:\n",
    "    if col.lower() in ['personality', 'target', 'label', 'class']:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col:\n",
    "    print(f'=== Target Variable Analysis: {target_col} ===')\n",
    "    print('\\nValue counts:')\n",
    "    print(train_df[target_col].value_counts())\n",
    "    \n",
    "    print('\\nPercentage distribution:')\n",
    "    print(train_df[target_col].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    # Plot target distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Count plot\n",
    "    train_df[target_col].value_counts().plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title(f'Distribution of {target_col}')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Pie chart\n",
    "    train_df[target_col].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%')\n",
    "    axes[1].set_title(f'Percentage Distribution of {target_col}')\n",
    "    axes[1].set_ylabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Target column not found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-analysis",
   "metadata": {
    "id": "feature-analysis"
   },
   "outputs": [],
   "source": [
    "# Feature analysis\n",
    "print('=== Feature Analysis ===')\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target from features if it's in the lists\n",
    "if target_col in numeric_features:\n",
    "    numeric_features.remove(target_col)\n",
    "if target_col in categorical_features:\n",
    "    categorical_features.remove(target_col)\n",
    "\n",
    "print(f'Numeric features ({len(numeric_features)}): {numeric_features}')\n",
    "print(f'Categorical features ({len(categorical_features)}): {categorical_features}')\n",
    "\n",
    "# Analyze numeric features\n",
    "if len(numeric_features) > 0:\n",
    "    print('\\nNumeric features statistics:')\n",
    "    display(train_df[numeric_features].describe())\n",
    "\n",
    "# Analyze categorical features\n",
    "if len(categorical_features) > 0:\n",
    "    print('\\nCategorical features info:')\n",
    "    for feature in categorical_features:\n",
    "        unique_count = train_df[feature].nunique()\n",
    "        print(f'{feature}: {unique_count} unique values')\n",
    "        if unique_count <= 10:\n",
    "            print(f'  Values: {train_df[feature].unique().tolist()}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributions",
   "metadata": {
    "id": "distributions"
   },
   "outputs": [],
   "source": [
    "# Plot feature distributions\n",
    "if len(numeric_features) > 0:\n",
    "    print('=== Numeric Feature Distributions ===')\n",
    "    \n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_features = len(numeric_features)\n",
    "    n_cols = min(3, n_features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    if n_features == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(numeric_features):\n",
    "        train_df[feature].hist(bins=30, ax=axes[i], alpha=0.7)\n",
    "        axes[i].set_title(f'Distribution of {feature}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-analysis",
   "metadata": {
    "id": "correlation-analysis"
   },
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "if len(numeric_features) > 1:\n",
    "    print('=== Correlation Analysis ===')\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = train_df[numeric_features].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, square=True, fmt='.2f')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated features\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.7:\n",
    "                high_corr_pairs.append((\n",
    "                    correlation_matrix.columns[i], \n",
    "                    correlation_matrix.columns[j], \n",
    "                    corr_val\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print('\\nHighly correlated feature pairs (|correlation| > 0.7):')\n",
    "        for feat1, feat2, corr in high_corr_pairs:\n",
    "            print(f'{feat1} - {feat2}: {corr:.3f}')\n",
    "    else:\n",
    "        print('\\nNo highly correlated feature pairs found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-engineering",
   "metadata": {
    "id": "feature-engineering"
   },
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Since we're in Colab, we'll implement the feature engineering directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-engineering-class",
   "metadata": {
    "id": "feature-engineering-class"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class FeatureEngineer:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.preprocessor = None\n",
    "        self.feature_selector = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def preprocess_data(self, X, y=None):\n",
    "        \"\"\"Preprocess the data\"\"\"\n",
    "        # Separate numeric and categorical features\n",
    "        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        # Create preprocessing pipelines\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "        \n",
    "        # Combine preprocessing steps\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ])\n",
    "        \n",
    "        # Fit and transform features\n",
    "        X_processed = self.preprocessor.fit_transform(X)\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = []\n",
    "        feature_names.extend(numeric_features)\n",
    "        if categorical_features:\n",
    "            cat_feature_names = self.preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
    "            feature_names.extend(cat_feature_names)\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # Process target variable if provided\n",
    "        if y is not None:\n",
    "            if y.dtype == 'object':\n",
    "                y_processed = self.label_encoder.fit_transform(y)\n",
    "            else:\n",
    "                y_processed = y.values\n",
    "            return X_processed, y_processed\n",
    "        \n",
    "        return X_processed\n",
    "    \n",
    "    def transform_features(self, X):\n",
    "        \"\"\"Transform features using fitted preprocessor\"\"\"\n",
    "        if self.preprocessor is None:\n",
    "            raise ValueError(\"Preprocessor not fitted. Call preprocess_data first.\")\n",
    "        return self.preprocessor.transform(X)\n",
    "    \n",
    "    def select_features(self, X, y, k=50):\n",
    "        \"\"\"Select top k features\"\"\"\n",
    "        self.feature_selector = SelectKBest(score_func=f_classif, k=min(k, X.shape[1]))\n",
    "        X_selected = self.feature_selector.fit_transform(X, y)\n",
    "        return X_selected\n",
    "\n",
    "print('FeatureEngineer class defined successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess-data",
   "metadata": {
    "id": "preprocess-data"
   },
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer()\n",
    "\n",
    "# Preprocess the data\n",
    "print('=== Feature Engineering ===')\n",
    "\n",
    "# Separate features and target\n",
    "if target_col:\n",
    "    X = train_df.drop(columns=[target_col])\n",
    "    y = train_df[target_col]\n",
    "    \n",
    "    print(f'Original feature shape: {X.shape}')\n",
    "    print(f'Target shape: {y.shape}')\n",
    "    \n",
    "    # Apply feature engineering\n",
    "    X_processed, y_processed = feature_engineer.preprocess_data(X, y)\n",
    "    \n",
    "    print(f'Processed feature shape: {X_processed.shape}')\n",
    "    print(f'Processed target shape: {y_processed.shape}')\n",
    "    \n",
    "    # Process test data\n",
    "    X_test_processed = feature_engineer.transform_features(test_df)\n",
    "    print(f'Processed test shape: {X_test_processed.shape}')\n",
    "    \n",
    "    # Feature selection\n",
    "    X_selected = feature_engineer.select_features(X_processed, y_processed, k=50)\n",
    "    print(f'Selected features shape: {X_selected.shape}')\n",
    "else:\n",
    "    print('Cannot proceed without target column!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-training",
   "metadata": {
    "id": "model-training"
   },
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-imports",
   "metadata": {
    "id": "model-imports"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "print('Model libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split",
   "metadata": {
    "id": "train-test-split"
   },
   "outputs": [],
   "source": [
    "# Split data for training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_selected, y_processed, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_processed\n",
    ")\n",
    "\n",
    "print(f'Training set shape: {X_train.shape}')\n",
    "print(f'Validation set shape: {X_val.shape}')\n",
    "print(f'Training target distribution:')\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "print(f'\\nValidation target distribution:')\n",
    "print(pd.Series(y_val).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-models",
   "metadata": {
    "id": "define-models"
   },
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss'),\n",
    "    'LightGBM': lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "print('Models defined successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-validation",
   "metadata": {
    "id": "cross-validation"
   },
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "print('=== Cross-Validation Results ===')\n",
    "cv_results = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\nEvaluating {name}...')\n",
    "    scores = cross_val_score(model, X_selected, y_processed, cv=cv, scoring='accuracy')\n",
    "    cv_results[name] = {\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std(),\n",
    "        'scores': scores\n",
    "    }\n",
    "    print(f'{name}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})')\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(cv_results).T\n",
    "results_df = results_df.sort_values('mean', ascending=False)\n",
    "print('\\n=== Final CV Results ===')\n",
    "display(results_df[['mean', 'std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-best-model",
   "metadata": {
    "id": "train-best-model"
   },
   "outputs": [],
   "source": [
    "# Train the best model\n",
    "best_model_name = results_df.index[0]\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f'=== Training Best Model: {best_model_name} ===')\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble-model",
   "metadata": {
    "id": "ensemble-model"
   },
   "outputs": [],
   "source": [
    "# Create ensemble model\n",
    "print('=== Creating Ensemble Model ===')\n",
    "\n",
    "# Select top 3 models\n",
    "top_3_models = results_df.head(3)\n",
    "ensemble_models = []\n",
    "\n",
    "for model_name in top_3_models.index:\n",
    "    ensemble_models.append((model_name.lower().replace(' ', '_'), models[model_name]))\n",
    "\n",
    "# Create voting classifier\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=ensemble_models,\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Train ensemble\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate ensemble on validation set\n",
    "y_pred_ensemble = ensemble_model.predict(X_val)\n",
    "ensemble_accuracy = accuracy_score(y_val, y_pred_ensemble)\n",
    "\n",
    "print(f'Ensemble Validation Accuracy: {ensemble_accuracy:.4f}')\n",
    "print(f'Best Single Model Accuracy: {accuracy:.4f}')\n",
    "print(f'Improvement: {ensemble_accuracy - accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictions",
   "metadata": {
    "id": "predictions"
   },
   "source": [
    "## 6. Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-predictions",
   "metadata": {
    "id": "final-predictions"
   },
   "outputs": [],
   "source": [
    "# Make final predictions on test set\n",
    "print('=== Generating Final Predictions ===')\n",
    "\n",
    "# Use the better performing model\n",
    "final_model = ensemble_model if ensemble_accuracy > accuracy else best_model\n",
    "model_name = 'Ensemble' if ensemble_accuracy > accuracy else f'Best Single Model ({best_model_name})'\n",
    "\n",
    "print(f'Using {model_name} for final predictions')\n",
    "\n",
    "# Apply feature selection to test data\n",
    "X_test_selected = feature_engineer.feature_selector.transform(X_test_processed)\n",
    "\n",
    "# Generate predictions\n",
    "test_predictions = final_model.predict(X_test_selected)\n",
    "\n",
    "# Convert predictions back to original labels if needed\n",
    "if hasattr(feature_engineer.label_encoder, 'classes_'):\n",
    "    test_predictions_labels = feature_engineer.label_encoder.inverse_transform(test_predictions)\n",
    "else:\n",
    "    test_predictions_labels = test_predictions\n",
    "\n",
    "print(f'Generated {len(test_predictions_labels)} predictions')\n",
    "print(f'Prediction distribution:')\n",
    "print(pd.Series(test_predictions_labels).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-submission",
   "metadata": {
    "id": "create-submission"
   },
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = sample_submission.copy()\n",
    "submission[target_col] = test_predictions_labels\n",
    "\n",
    "print('Submission preview:')\n",
    "display(submission.head(10))\n",
    "\n",
    "print(f'\\nSubmission shape: {submission.shape}')\n",
    "print(f'Submission target distribution:')\n",
    "print(submission[target_col].value_counts(normalize=True))\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('\\nSubmission saved as submission.csv')\n",
    "\n",
    "# Download the submission file\n",
    "files.download('submission.csv')\n",
    "print('Submission file downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This Google Colab notebook provided a complete analysis pipeline for the Kaggle Introvert vs Extrovert classification competition:\n",
    "\n",
    "### Key Features:\n",
    "- **Cloud-optimized**: Designed specifically for Google Colab environment\n",
    "- **Data Upload**: Multiple methods to upload your competition data\n",
    "- **Complete EDA**: Comprehensive exploratory data analysis with visualizations\n",
    "- **Feature Engineering**: Built-in preprocessing and feature selection\n",
    "- **Model Training**: Multiple algorithms with cross-validation\n",
    "- **Ensemble Methods**: Automatic ensemble creation for better performance\n",
    "- **Easy Download**: Automatic download of submission file\n",
    "\n",
    "### Results:\n",
    "- Trained and evaluated multiple machine learning models\n",
    "- Created an ensemble model for improved performance\n",
    "- Generated final predictions ready for Kaggle submission\n",
    "\n",
    "### Next Steps:\n",
    "1. Download the `submission.csv` file\n",
    "2. Upload to Kaggle competition\n",
    "3. Experiment with different hyperparameters or feature engineering techniques\n",
    "4. Try advanced ensemble methods or neural networks\n",
    "\n",
    "**Happy Kaggling! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}