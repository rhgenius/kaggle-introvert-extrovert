{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "main-title",
   "metadata": {},
   "source": [
    "# Kaggle Introvert vs Extrovert Classification\n",
    "\n",
    "This notebook provides a complete analysis and modeling pipeline for the Kaggle Introvert vs Extrovert classification competition.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading and Setup](#data-loading)\n",
    "2. [Exploratory Data Analysis](#eda)\n",
    "3. [Feature Engineering](#feature-engineering)\n",
    "4. [Model Training and Evaluation](#model-training)\n",
    "5. [Final Predictions](#predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = Path('../data/raw')\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(data_path / 'train.csv')\n",
    "    test_df = pd.read_csv(data_path / 'test.csv')\n",
    "    sample_submission = pd.read_csv(data_path / 'sample_submission.csv')\n",
    "    \n",
    "    print('Data loaded successfully!')\n",
    "    print(f'Training data shape: {train_df.shape}')\n",
    "    print(f'Test data shape: {test_df.shape}')\n",
    "    print(f'Sample submission shape: {sample_submission.shape}')\n",
    "except FileNotFoundError as e:\n",
    "    print(f'Error loading data: {e}')\n",
    "    print('Please ensure data files are in the ../data/raw/ directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print('=== Dataset Overview ===')\n",
    "print('\\nTraining Data Info:')\n",
    "print(train_df.info())\n",
    "\n",
    "print('\\nFirst few rows of training data:')\n",
    "display(train_df.head())\n",
    "\n",
    "print('\\nBasic statistics:')\n",
    "display(train_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print('=== Missing Values Analysis ===')\n",
    "missing_train = train_df.isnull().sum()\n",
    "missing_test = test_df.isnull().sum()\n",
    "\n",
    "print('\\nMissing values in training data:')\n",
    "print(missing_train[missing_train > 0])\n",
    "\n",
    "print('\\nMissing values in test data:')\n",
    "print(missing_test[missing_test > 0])\n",
    "\n",
    "# Visualize missing values\n",
    "if missing_train.sum() > 0 or missing_test.sum() > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Training data missing values\n",
    "    missing_train_pct = (missing_train / len(train_df)) * 100\n",
    "    missing_train_pct = missing_train_pct[missing_train_pct > 0].sort_values(ascending=False)\n",
    "    if len(missing_train_pct) > 0:\n",
    "        missing_train_pct.plot(kind='bar', ax=axes[0])\n",
    "        axes[0].set_title('Missing Values in Training Data (%)')\n",
    "        axes[0].set_ylabel('Percentage')\n",
    "    \n",
    "    # Test data missing values\n",
    "    missing_test_pct = (missing_test / len(test_df)) * 100\n",
    "    missing_test_pct = missing_test_pct[missing_test_pct > 0].sort_values(ascending=False)\n",
    "    if len(missing_test_pct) > 0:\n",
    "        missing_test_pct.plot(kind='bar', ax=axes[1])\n",
    "        axes[1].set_title('Missing Values in Test Data (%)')\n",
    "        axes[1].set_ylabel('Percentage')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No missing values found in the datasets!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable analysis\n",
    "target_col = None\n",
    "for col in train_df.columns:\n",
    "    if col.lower() in ['personality', 'target', 'label', 'class']:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col:\n",
    "    print(f'=== Target Variable Analysis: {target_col} ===')\n",
    "    print('\\nValue counts:')\n",
    "    print(train_df[target_col].value_counts())\n",
    "    \n",
    "    print('\\nPercentage distribution:')\n",
    "    print(train_df[target_col].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    # Plot target distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Count plot\n",
    "    train_df[target_col].value_counts().plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title(f'Distribution of {target_col}')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Pie chart\n",
    "    train_df[target_col].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%')\n",
    "    axes[1].set_title(f'Percentage Distribution of {target_col}')\n",
    "    axes[1].set_ylabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Target column not found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature analysis\n",
    "print('=== Feature Analysis ===')\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target from features if it's in the lists\n",
    "if target_col in numeric_features:\n",
    "    numeric_features.remove(target_col)\n",
    "if target_col in categorical_features:\n",
    "    categorical_features.remove(target_col)\n",
    "\n",
    "print(f'Numeric features ({len(numeric_features)}): {numeric_features}')\n",
    "print(f'Categorical features ({len(categorical_features)}): {categorical_features}')\n",
    "\n",
    "# Analyze numeric features\n",
    "if len(numeric_features) > 0:\n",
    "    print('\\nNumeric features statistics:')\n",
    "    display(train_df[numeric_features].describe())\n",
    "\n",
    "# Analyze categorical features\n",
    "if len(categorical_features) > 0:\n",
    "    print('\\nCategorical features info:')\n",
    "    for feature in categorical_features:\n",
    "        unique_count = train_df[feature].nunique()\n",
    "        print(f'{feature}: {unique_count} unique values')\n",
    "        if unique_count <= 10:\n",
    "            print(f'  Values: {train_df[feature].unique().tolist()}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature distributions\n",
    "if len(numeric_features) > 0:\n",
    "    print('=== Numeric Feature Distributions ===')\n",
    "    \n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_features = len(numeric_features)\n",
    "    n_cols = min(3, n_features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    if n_features == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(numeric_features):\n",
    "        train_df[feature].hist(bins=30, ax=axes[i], alpha=0.7)\n",
    "        axes[i].set_title(f'Distribution of {feature}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "if len(numeric_features) > 1:\n",
    "    print('=== Correlation Analysis ===')\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = train_df[numeric_features].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, square=True, fmt='.2f')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated features\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.7:\n",
    "                high_corr_pairs.append((\n",
    "                    correlation_matrix.columns[i], \n",
    "                    correlation_matrix.columns[j], \n",
    "                    corr_val\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print('\\nHighly correlated feature pairs (|correlation| > 0.7):')\n",
    "        for feat1, feat2, corr in high_corr_pairs:\n",
    "            print(f'{feat1} - {feat2}: {corr:.3f}')\n",
    "    else:\n",
    "        print('\\nNo highly correlated feature pairs found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-engineering",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-engineering-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import feature engineering module\n",
    "from feature_engineering import FeatureEngineer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer()\n",
    "\n",
    "print('Feature engineering module loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "print('=== Feature Engineering ===')\n",
    "\n",
    "# Separate features and target\n",
    "if target_col:\n",
    "    X = train_df.drop(columns=[target_col])\n",
    "    y = train_df[target_col]\n",
    "    \n",
    "    print(f'Original feature shape: {X.shape}')\n",
    "    print(f'Target shape: {y.shape}')\n",
    "    \n",
    "    # Apply feature engineering\n",
    "    X_processed, y_processed = feature_engineer.preprocess_data(X, y)\n",
    "    \n",
    "    print(f'Processed feature shape: {X_processed.shape}')\n",
    "    print(f'Processed target shape: {y_processed.shape}')\n",
    "    \n",
    "    # Process test data\n",
    "    X_test_processed = feature_engineer.transform_features(test_df)\n",
    "    print(f'Processed test shape: {X_test_processed.shape}')\n",
    "else:\n",
    "    print('Cannot proceed without target column!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-training",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model training module\n",
    "from model_training import ModelTrainer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize model trainer\n",
    "model_trainer = ModelTrainer(random_state=42)\n",
    "\n",
    "print('Model training module loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_processed, y_processed, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_processed\n",
    ")\n",
    "\n",
    "print(f'Training set shape: {X_train.shape}')\n",
    "print(f'Validation set shape: {X_val.shape}')\n",
    "print(f'Training target distribution:')\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "print(f'\\nValidation target distribution:')\n",
    "print(pd.Series(y_val).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "print('=== Cross-Validation Results ===')\n",
    "cv_scores = model_trainer.cross_validate_models(X_processed, y_processed)\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(cv_scores).T\n",
    "results_df = results_df.sort_values('mean', ascending=False)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-best-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model\n",
    "print('=== Training Best Model ===')\n",
    "best_model = model_trainer.train_best_model(X_train, y_train)\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble model\n",
    "print('=== Creating Ensemble Model ===')\n",
    "ensemble_model = model_trainer.create_ensemble(X_processed, y_processed)\n",
    "\n",
    "# Evaluate ensemble on validation set\n",
    "y_pred_ensemble = ensemble_model.predict(X_val)\n",
    "ensemble_accuracy = accuracy_score(y_val, y_pred_ensemble)\n",
    "\n",
    "print(f'Ensemble Validation Accuracy: {ensemble_accuracy:.4f}')\n",
    "print(f'Single Model Accuracy: {accuracy:.4f}')\n",
    "print(f'Improvement: {ensemble_accuracy - accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictions",
   "metadata": {},
   "source": [
    "## 5. Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final predictions on test set\n",
    "print('=== Generating Final Predictions ===')\n",
    "\n",
    "# Use the better performing model\n",
    "final_model = ensemble_model if ensemble_accuracy > accuracy else best_model\n",
    "model_name = 'Ensemble' if ensemble_accuracy > accuracy else 'Best Single Model'\n",
    "\n",
    "print(f'Using {model_name} for final predictions')\n",
    "\n",
    "# Generate predictions\n",
    "test_predictions = final_model.predict(X_test_processed)\n",
    "\n",
    "print(f'Generated {len(test_predictions)} predictions')\n",
    "print(f'Prediction distribution:')\n",
    "print(pd.Series(test_predictions).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = sample_submission.copy()\n",
    "submission[target_col] = test_predictions\n",
    "\n",
    "# Save submission\n",
    "submission_path = Path('../data/processed/submission.csv')\n",
    "submission_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f'Submission saved to: {submission_path}')\n",
    "print('\\nSubmission preview:')\n",
    "display(submission.head(10))\n",
    "\n",
    "print(f'\\nSubmission shape: {submission.shape}')\n",
    "print(f'Submission target distribution:')\n",
    "print(submission[target_col].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provided a complete analysis pipeline for the Kaggle Introvert vs Extrovert classification competition:\n",
    "\n",
    "1. **Data Loading**: Successfully loaded training, test, and submission data\n",
    "2. **EDA**: Analyzed data structure, missing values, target distribution, and feature relationships\n",
    "3. **Feature Engineering**: Applied preprocessing, feature creation, and scaling\n",
    "4. **Model Training**: Trained multiple models, performed cross-validation, and created ensemble\n",
    "5. **Predictions**: Generated final predictions and created submission file\n",
    "\n",
    "The final model achieved good performance and the submission is ready for upload to Kaggle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}