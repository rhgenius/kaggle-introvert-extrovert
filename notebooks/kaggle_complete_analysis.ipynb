{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "kaggle-title",
   "metadata": {},
   "source": [
    "# Kaggle Introvert vs Extrovert Classification - Kaggle Notebook\n",
    "\n",
    "This notebook provides a complete analysis and modeling pipeline for the Kaggle Playground Series S5E7 competition, optimized for Kaggle's environment.\n",
    "\n",
    "## Competition: Predict the Introverts from the Extroverts\n",
    "- Competition URL: https://www.kaggle.com/competitions/playground-series-s5e7\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Data Loading](#data-loading)\n",
    "3. [Exploratory Data Analysis](#eda)\n",
    "4. [Feature Engineering](#feature-engineering)\n",
    "5. [Model Training and Evaluation](#model-training)\n",
    "6. [Final Predictions](#predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Kaggle notebooks come with most packages pre-installed, but we'll import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "In Kaggle notebooks, competition data is automatically available in the `/kaggle/input/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle data paths\n",
    "data_path = Path('/kaggle/input/playground-series-s5e7')\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(data_path / 'train.csv')\n",
    "test_df = pd.read_csv(data_path / 'test.csv')\n",
    "sample_submission = pd.read_csv(data_path / 'sample_submission.csv')\n",
    "\n",
    "print('Data loaded successfully!')\n",
    "print(f'Training data shape: {train_df.shape}')\n",
    "print(f'Test data shape: {test_df.shape}')\n",
    "print(f'Sample submission shape: {sample_submission.shape}')\n",
    "\n",
    "# Display first few rows\n",
    "print('\\n=== Training Data Sample ===')\n",
    "display(train_df.head())\n",
    "\n",
    "print('\\n=== Test Data Sample ===')\n",
    "display(test_df.head())\n",
    "\n",
    "print('\\n=== Sample Submission ===')\n",
    "display(sample_submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print('=== Dataset Info ===')\n",
    "print('\\n--- Training Data Info ---')\n",
    "print(train_df.info())\n",
    "\n",
    "print('\\n--- Test Data Info ---')\n",
    "print(test_df.info())\n",
    "\n",
    "print('\\n--- Missing Values ---')\n",
    "print('Training data missing values:')\n",
    "print(train_df.isnull().sum())\n",
    "print('\\nTest data missing values:')\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "print('\\n--- Data Types ---')\n",
    "print('Training data types:')\n",
    "print(train_df.dtypes)\n",
    "print('\\nTest data types:')\n",
    "print(test_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable analysis\n",
    "target_col = 'Personality'  # Adjust if different\n",
    "\n",
    "if target_col in train_df.columns:\n",
    "    print(f'=== Target Variable: {target_col} ===')\n",
    "    print(train_df[target_col].value_counts())\n",
    "    print(f'\\nTarget distribution (%):')\n",
    "    print(train_df[target_col].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    # Plot target distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    train_df[target_col].value_counts().plot(kind='bar')\n",
    "    plt.title(f'{target_col} Distribution (Count)')\n",
    "    plt.xlabel(target_col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    train_df[target_col].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "    plt.title(f'{target_col} Distribution (%)')\n",
    "    plt.ylabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Target column not found. Available columns:')\n",
    "    print(train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature analysis\n",
    "print('=== Feature Analysis ===')\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target_col in numeric_features:\n",
    "    numeric_features.remove(target_col)\n",
    "\n",
    "categorical_features = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "if target_col in categorical_features:\n",
    "    categorical_features.remove(target_col)\n",
    "\n",
    "print(f'Numeric features ({len(numeric_features)}): {numeric_features}')\n",
    "print(f'Categorical features ({len(categorical_features)}): {categorical_features}')\n",
    "\n",
    "# Statistical summary for numeric features\n",
    "if len(numeric_features) > 0:\n",
    "    print('\\n=== Numeric Features Summary ===')\n",
    "    display(train_df[numeric_features].describe())\n",
    "\n",
    "# Categorical features summary\n",
    "if len(categorical_features) > 0:\n",
    "    print('\\n=== Categorical Features Summary ===')\n",
    "    for feature in categorical_features:\n",
    "        print(f'\\n{feature}:')\n",
    "        print(train_df[feature].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-distributions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature distributions\n",
    "if len(numeric_features) > 0:\n",
    "    n_features = min(len(numeric_features), 12)  # Limit to 12 features\n",
    "    n_cols = 4\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    plt.figure(figsize=(20, 5 * n_rows))\n",
    "    \n",
    "    for i, feature in enumerate(numeric_features[:n_features]):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        train_df[feature].hist(bins=30, alpha=0.7)\n",
    "        plt.title(f'{feature} Distribution')\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "if len(numeric_features) > 1:\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    correlation_matrix = train_df[numeric_features].corr()\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    # Generate heatmap\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated features\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "                high_corr_pairs.append((\n",
    "                    correlation_matrix.columns[i],\n",
    "                    correlation_matrix.columns[j],\n",
    "                    correlation_matrix.iloc[i, j]\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print('\\n=== Highly Correlated Features (|r| > 0.8) ===')\n",
    "        for feat1, feat2, corr in high_corr_pairs:\n",
    "            print(f'{feat1} - {feat2}: {corr:.3f}')\n",
    "    else:\n",
    "        print('\\nNo highly correlated features found (|r| > 0.8)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-engineering",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-engineering-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_selector = None\n",
    "        self.target_encoder = LabelEncoder()\n",
    "    \n",
    "    def preprocess_data(self, train_df, test_df, target_col='Personality'):\n",
    "        \"\"\"Preprocess training and test data\"\"\"\n",
    "        print('Starting feature engineering...')\n",
    "        \n",
    "        # Separate features and target\n",
    "        X_train = train_df.drop(columns=[target_col])\n",
    "        y_train = train_df[target_col].copy()\n",
    "        X_test = test_df.copy()\n",
    "        \n",
    "        # Remove ID column if present\n",
    "        id_cols = ['id', 'Id', 'ID']\n",
    "        for col in id_cols:\n",
    "            if col in X_train.columns:\n",
    "                X_train = X_train.drop(columns=[col])\n",
    "            if col in X_test.columns:\n",
    "                X_test = X_test.drop(columns=[col])\n",
    "        \n",
    "        # Encode target variable\n",
    "        y_train_encoded = self.target_encoder.fit_transform(y_train)\n",
    "        \n",
    "        # Handle categorical features\n",
    "        categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        for feature in categorical_features:\n",
    "            # Fill missing values\n",
    "            X_train[feature] = X_train[feature].fillna('Unknown')\n",
    "            X_test[feature] = X_test[feature].fillna('Unknown')\n",
    "            \n",
    "            # Label encoding\n",
    "            self.label_encoders[feature] = LabelEncoder()\n",
    "            \n",
    "            # Fit on combined data to handle unseen categories\n",
    "            combined_values = pd.concat([X_train[feature], X_test[feature]])\n",
    "            self.label_encoders[feature].fit(combined_values)\n",
    "            \n",
    "            X_train[feature] = self.label_encoders[feature].transform(X_train[feature])\n",
    "            X_test[feature] = self.label_encoders[feature].transform(X_test[feature])\n",
    "        \n",
    "        # Handle numeric features\n",
    "        numeric_features = X_train.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        # Fill missing values with median\n",
    "        for feature in numeric_features:\n",
    "            median_val = X_train[feature].median()\n",
    "            X_train[feature] = X_train[feature].fillna(median_val)\n",
    "            X_test[feature] = X_test[feature].fillna(median_val)\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Convert back to DataFrame\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "        \n",
    "        print(f'Feature engineering completed!')\n",
    "        print(f'Training features shape: {X_train_scaled.shape}')\n",
    "        print(f'Test features shape: {X_test_scaled.shape}')\n",
    "        \n",
    "        return X_train_scaled, X_test_scaled, y_train_encoded\n",
    "    \n",
    "    def select_features(self, X_train, y_train, k=50):\n",
    "        \"\"\"Select top k features\"\"\"\n",
    "        self.feature_selector = SelectKBest(score_func=f_classif, k=min(k, X_train.shape[1]))\n",
    "        X_train_selected = self.feature_selector.fit_transform(X_train, y_train)\n",
    "        \n",
    "        selected_features = X_train.columns[self.feature_selector.get_support()]\n",
    "        print(f'Selected {len(selected_features)} features: {selected_features.tolist()}')\n",
    "        \n",
    "        return X_train_selected, selected_features\n",
    "    \n",
    "    def transform_test(self, X_test):\n",
    "        \"\"\"Transform test data using fitted feature selector\"\"\"\n",
    "        if self.feature_selector is not None:\n",
    "            return self.feature_selector.transform(X_test)\n",
    "        return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-feature-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering\n",
    "fe = FeatureEngineer()\n",
    "X_train, X_test, y_train = fe.preprocess_data(train_df, test_df, target_col='Personality')\n",
    "\n",
    "# Feature selection (optional)\n",
    "# X_train_selected, selected_features = fe.select_features(X_train, y_train, k=50)\n",
    "# X_test_selected = fe.transform_test(X_test)\n",
    "\n",
    "print('\\nFeature engineering completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-training",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-training-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.best_model = None\n",
    "        self.best_score = 0\n",
    "    \n",
    "    def train_models(self, X_train, y_train, cv_folds=5):\n",
    "        \"\"\"Train multiple models and compare performance\"\"\"\n",
    "        print('Training models...')\n",
    "        \n",
    "        # Define models\n",
    "        models = {\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "            'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "            'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "        }\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f'Training {name}...')\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "            results[name] = {\n",
    "                'mean': scores.mean(),\n",
    "                'std': scores.std(),\n",
    "                'scores': scores\n",
    "            }\n",
    "            \n",
    "            # Fit model on full training data\n",
    "            model.fit(X_train, y_train)\n",
    "            self.models[name] = model\n",
    "            \n",
    "            # Track best model\n",
    "            if results[name]['mean'] > self.best_score:\n",
    "                self.best_score = results[name]['mean']\n",
    "                self.best_model = model\n",
    "        \n",
    "        # Display results\n",
    "        print('\\n=== Model Performance (Cross-Validation) ===')\n",
    "        for name, result in results.items():\n",
    "            print(f'{name}: {result[\"mean\"]:.4f} (+/- {result[\"std\"] * 2:.4f})')\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_model(self, X_val, y_val, model_name=None):\n",
    "        \"\"\"Evaluate model on validation set\"\"\"\n",
    "        if model_name:\n",
    "            model = self.models[model_name]\n",
    "        else:\n",
    "            model = self.best_model\n",
    "            model_name = 'Best Model'\n",
    "        \n",
    "        y_pred = model.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "        print(f'\\n=== {model_name} Evaluation ===')\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "        print('\\nClassification Report:')\n",
    "        print(classification_report(y_val, y_pred))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'{model_name} - Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def predict(self, X_test, model_name=None):\n",
    "        \"\"\"Make predictions on test set\"\"\"\n",
    "        if model_name:\n",
    "            model = self.models[model_name]\n",
    "        else:\n",
    "            model = self.best_model\n",
    "        \n",
    "        return model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for validation\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train_split.shape}')\n",
    "print(f'Validation set: {X_val_split.shape}')\n",
    "\n",
    "# Train models\n",
    "trainer = ModelTrainer()\n",
    "cv_results = trainer.train_models(X_train_split, y_train_split)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_accuracy = trainer.evaluate_model(X_val_split, y_val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictions",
   "metadata": {},
   "source": [
    "## 6. Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain best model on full training data\n",
    "print('Retraining best model on full training data...')\n",
    "trainer.best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = trainer.predict(X_test)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "test_predictions_labels = fe.target_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "print(f'Test predictions shape: {test_predictions.shape}')\n",
    "print(f'Prediction distribution:')\n",
    "unique, counts = np.unique(test_predictions_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f'{label}: {count} ({count/len(test_predictions_labels)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = sample_submission.copy()\n",
    "submission['Personality'] = test_predictions_labels\n",
    "\n",
    "# Save submission\n",
    "submission_filename = 'submission.csv'\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f'Submission saved as {submission_filename}')\n",
    "print('\\nSubmission preview:')\n",
    "display(submission.head(10))\n",
    "\n",
    "print(f'\\nSubmission shape: {submission.shape}')\n",
    "print(f'Submission file size: {os.path.getsize(submission_filename)} bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print('=== Analysis Summary ===')\n",
    "print(f'Dataset: {train_df.shape[0]} training samples, {test_df.shape[0]} test samples')\n",
    "print(f'Features: {X_train.shape[1]} features after preprocessing')\n",
    "print(f'Best model validation accuracy: {val_accuracy:.4f}')\n",
    "print(f'Submission file: {submission_filename}')\n",
    "print('\\nReady for submission to Kaggle!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}